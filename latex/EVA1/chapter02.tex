%This is chapter 2
%%=========================================
\chapter{Re* research characteristics}
\section{Repeatability}
The repetition by Tomas Kalibera\cite{DBLP:conf/popl/Vitek15} definition is "the possibility of repeating the same experiment, simulation or scientific method in the same system and way and obtaining the same result." This requirement applies more to researchers to ensure stable results, but is also necessary for the scientific community in particular. Having the opportunity to repeat the result leads to new ideas. When the results are repetitive, it makes it easier for systems research. This characteristic is beneficial for scientific reviewers since those works/results that are repetitive and provide a code or web interface in the public domain can be tested for reliability. And also to evaluate the proposed method, the change in input data can be used. Especially this applies to those works for which no special hardware or software infrastructure is required. \par
Another author Collberg\cite{DBLP:journals/cacm/CollbergP16} defines the repeated study as " if it is possible to re-run the researchers experiment using the same method in the same environment and obtain the same results." In his opinion, it is necessary to keep results repeatable that reviewers, colleagues and other scientists could evaluate the outcome and methodology by full and accurate evidence. With this approach, a lot of repetitive work will be avoided; the researchers rely on the result and scientific progress will advance. \par
In this work, repeatability can be defined as an ability of scientific work to be repeated the various number of times and receive the entirely the same result of each experiment. The researcher needs to provide full information about his experimental set-up, software, codebase and initial datasets with instructions how to run them. This workflow is shown in the picture \ref{fig:repeatability}. \par
\begin{figure}[h!]
  \includegraphics[scale=0.8]{fig/repeatability.png}
  \caption{Repeatability\cite{gith}}
  \label{fig:repeatability}
\end{figure}
\subsection{Current state of art}
The National Science Foundation Grant Proposal Guide\cite{nfsgp} says, “Investigators and grantees are encouraged to share software and inventions created under the grant or otherwise make them or their products widely available and usable.” 
Typically, papers from industry have low repeatability because most of the code is under a private license, at the same time academic works, because they use open-source code, have a higher degree of repeatability.
\subsection{Recommendations}
\begin{itemize}
    \item One of the most widely used practices to ensure scientific work is repeatable is the packaging of all source data, code, operating system, and third-party software into a \textbf{virtual image}. This practice will allow the project to be run by any researcher and solve problems with the versioning. The more it leads to own issues and limitations that need to be resolved. Like how to solve the problem with the launching of a virtual image that may have had an outdated operating system or applications that have identified the security vulnerabilities and at the moment they are not patched, and still, results might be different because of hardware setup, etc.
    \item \textbf{Fund repeatability engineering}. Special agencies should encourage researchers to request additional funds for “repeatability engineering,” including hiring programming staff to document and maintain code, do release management, and assist other research groups wanting to repeat published experiments. In the same way funding agencies conduct financial audits to ensure costs claimed by grantees are allowed, they should also conduct random audits to ensure research artifacts are shared by what was promised in the grant application
    \item \textbf{Require sharing contract}. Publishers of conference proceedings and journals should require every article include a sharing contract specifying the level of repeatability to which its authors will commit. While the first point will have the effect of shifting some funding from pure research to engineering and oversight, both are important because they ensure research results continue to benefit the academic community— and the public funding it—past the project end date. 
    \item \textbf{Sharing contracts}. The sharing contract should be provided by the authors when a paper is submitted for publication (allowing reviewers to consider the expected level of repeatability of the work), as well as in the published version (allowing readers to locate research artifacts). The contract commits the author to make certain available resources that were used in the research leading up to the paper and committing the reader/reviewer to take these resources into account when evaluating the contributions made by the paper
\end{itemize}
\section{Reproducibility}
Many research departments recently have been increasingly worried by the reproducibility of scientific papers, especially in computation-based experiments, where computers and computations are the central part of the experiment. To ensure performance is necessary to publish all the computational tools, documentation, datasets and benchmarks that were used by the researchers for this experiment. For any researcher who would like to reproduce results and experiments open source code, documentation, datasets should be available. This movement in the scientific world has a de facto title "Reproducible research"\cite{DBLP:journals/jetai/Drummond18}. \par
In the work of Tomas Kalibera\cite{DBLP:conf/popl/Vitek15}, reproducibility is defined as: "Independent confirmation of a scientific hypothesis through reproduction by an independent researcher/lab is at the core of the scientific method." Reproduction of scientific work should be done by independent researchers after publishing the paper with available datasets, code, experimental setup, etc. Required data can also be requested from the author. It is also possible to say that in the case of error or deception of work reproducibility is better to use compared to repeatability since it provides a higher level of abstraction.\par
Comparing repeatability and reproducibility, another author Colberg\cite{DBLP:journals/cacm/CollbergP16} notes that reproducibility does not require an identical experimental setup with the same hardware. Instead, it is an independent confirmation of the scientific hypothesis/results that are performed after the publication by other research units conducting the same experiments with the same or similar data periodically changing them that come to the similar or the same conclusions. It can be concluded that: Repeatability and reproducibility are characteristics of the scientific workflow that is necessary to prevent the proliferation of erroneous results. \par
In this work, reproducibility can be defined as an ability of scientific work to be reproduced by any other research group not depending on hardware requirements. Compared to repeatability, results should not be identical but rather conclusions and data trends should be confirmed. A researcher needs to provide full information about codebase and initial datasets with instructions how to run them. Although an alternative solution that also confirms the conclusions can be considered reproducible. This workflow is shown in the picture \ref{fig:reproducability}.\par
\begin{figure}[h!]
  \includegraphics[scale=0.8]{fig/reproducibility.png}
  \caption{Reproducibility\cite{gith}}
  \label{fig:reproducability}
\end{figure}
\subsection{Current state of art}
Since the problem of the reproducibility of scientific works is not new, many scientific journals like PLOS and  Nature Methods with some governmental institutions such as NIH require authors to provide all the data obtained during the experiments. Data include documented steps of experiments and intermediate results, in order so that interested parties have access to this information, and there is an opportunity to promote similar research. Often, studies that reproduce already existing hypotheses are also published in journals whether they confirm whether or not the results of the original study. In the world of computer science, unfortunately, this does not happen, although there have been numerous attempts at the Reproducing research planet\cite{rrp}, and the Evaluation of Collaboratory\cite{evcol}. The reproducible research planet is a website for scientists who must store research data including data sets, code, documentation, experimental setup, etc. There have been attempts to make this website part of a standard research process in several articles and journals. In turn, the authors thereby increased their chance to be cited and reproducibility in general. The Evaluate Collaboratory is in addition to organizing workshops on experimental evaluation of software and computer systems, initiated a petition to program committee chairs of conferences and workshops that called for acceptance of reproduction studies as first-class publications.
\subsection{Recommendations}
\begin{itemize}
    \item \textbf{Develop open source benchmarks}. Required benchmarks, well documented, open source and carefully evaluated benchmarks should be fully accepted as first-class contributions for publications at leading conferences and journals and should be supported.
    \item \textbf{Codify best practice documentation, methodologies and reporting standards}. It is necessary to harmonize the minimum standards for documenting experiments and reporting. This can be done through community efforts and target working groups. It is also required to understand the factors that affect the measurable properties of computer systems, and better understand their statistical properties.
    \item \textbf{Require repeatability of published results}. Repeatability should be part of the publication review process. Based on the document and additional material on the experiments (documentation, configuration, source code, input sets, scripts), reviewers should make sure that the experiments are repeatable.
\end{itemize}
\section{Recomputability}
In work "The Recomputation Manifesto"\cite{DBLP:journals/corr/abs-1304-3674} Gent indicates that in the scientific development, replication is of great importance, but usually, in the computer science world, replication does not take much attention and researchers do not repeat the same experiment over and over again. And those experiments that are rarely implemented do this in full. The manifesto proposes to make the source code more accessible, although this is a small step. Usually, even when the source code can be launched this does not mean that the experiment can be repeated. In this paper, it is proposed that the discipline of computer science include the replication of experiments as a standard practice and also that tools and repositories are available..\par
Some of the authors\cite{DBLP:conf/psb/KaushikISTDK17} define recomputability the same as reproducibility as the ability to achieve the same results on the same data regardless of the computing environment or when the analysis is performed.\par
In his paper\cite{DBLP:conf/ucc/WehrleLVR14}, Wehrle notes that research is currently associated with highly specialized software and complex data processing workflows. While technology accelerates scientific progress, this acceleration also risks the sustainability of progress. Since almost every piece of information is created, transmitted and stored digitally, and the information is not necessarily associated with single, easily identifiable digital objects, but can be embedded in complex environments and processes, which allows to check, replicate and reuse data and accompanying methods not guaranteed, especially in the long run.\par
Matthias Kricke\cite{DBLP:journals/dbsk/KrickeGS17} provides one of the most accurate definitions of replication, this "ability to recompile results from raw data at any time" is essential for these studies to ensure data stability and selectively include new data in the already supplied data product. The competence of these products allows the client to restore earlier versions of data, reports and analysis results to compare them with new ones.\par
In this work, recomputability can be defined as an ability of scientific work to be recomputed any point in time with any data and by anyone who would like to do it. A researcher needs to provide full information about codebase and make it publicly available. Compare to reproducibility and repeatability the goal is not to get the same results or conclusions, but instead, other people can reuse codebase for any purposes. This workflow is shown in a picture \ref{fig:recomputability}.\par
\begin{figure}[h!]
  \includegraphics[scale=0.8]{fig/recomputability.png}
  \caption{Recomputability\cite{gith}}
  \label{fig:recomputability}
\end{figure}
\subsection{Current state of art}
Minimum information standards for recomputable experiments have been discussed and publically available for some years (e.g.,  MIAME\cite{MIAME}, MIABi\cite{MIABi}), although much recent discussion on the practical implementations of recomputability frequently takes place on the Internet and in blogs rather than in peer-reviewed literature\cite{ivory}\cite{recomputation}\cite{bioinformaticszen}. 
\subsection{Recommendations}
\begin{itemize}
    \item Each publication presenting computational results should be aimed at creating a "virtual reference environment" accompanying the publication. This reference environment should be the minimum implementation of the software stack needed to reproduce some or all of the computational part of the results. For a desktop tool, this will be the operating system, libraries, tools, and data. By downloading this image and placing it on a virtual machine monitor, such as VMWare \cite{vmware} or VirtualBox \cite{virtualbox}, readers and reviewers can immediately reproduce and examine the results with minimal setup effort. Many researchers now also have access to institutional or national "cloud" environments, and a virtual reference environment can easily be applied to one of them.
    \item Using open and free software, such a reference medium is now technically simple for production and distribution; it does not require specialized knowledge in the field of software development or system administration beyond the skills set of a typical bioinformatics researcher. If the result is operating systems and tools with open source code or software, there are step-by-step tools for creating a snapshot of the working environment (for example, Ubuntu Builder \cite{launchpad} or Relinux).
    \item Using already existing tools like Popper\cite{DBLP:conf/infocom/JimenezAALMMR17}, which goal is to implement executable papers in today's cloud-computing world by treating an article as an open source software (OSS) project. Popper is realized in the form of a convention for systematically implementing the different stages of the experimentation process following a DevOps approach.
\end{itemize}
\section{Reusability}
One of the complete works on the scientific works reusability belongs to Costantino Thanos\cite{DBLP:journals/publications/Thanos17} in his the term re-use of data means the ease of using data collected for one purpose, to study a new problem. This term implies re-use of separate data sets in the context of different contexts. Re-use of data becomes a unique feature of modern scientific practice, as it allows for the re-examination of evidence, reproduction, and verification of results, minimizing duplication of effort and relying on the work of others.\par 
In another paper, Gabriel Fils\cite{DBLP:journals/corr/ThatFYM17} say that the minimal use case for sharing a computational experiment (in the form of a joint research object) involves repeating its initial implementation and verifying its results. However, to use its potential, it must support modified reuse. The objective of research should be created and stored not as a simple aggregate of digital content, but in an efficiently computable form: as a reusable object.\par
In this work, reusability can be defined as an ability of scientific work to be reused any point of time with any data and by anyone who would like to do it and ultimately everything should be documented starting from the progress, small conclusions, ideas, methods, and benchmarks choice. A researcher needs to provide full information about codebase, datasets, and documentation. The main idea that other researcher can reuse the data to make brand new conclusions/papers. This workflow is shown in the picture \ref{fig:reusability}.\par
\begin{figure}[h!]
  \includegraphics[scale=0.8]{fig/reusability.png}
  \caption{Reusability\cite{gith}}
  \label{fig:reusability}
\end{figure}
\subsection{Current state of art}
The ability to reuse data can be efficiently implemented within Open Science since the ultimate goal of Open Science is to make research data public and reusable. The European Commission is moving decisively towards the implementation of the "Open Science" concept in Europe: in 2015, the European Commission called on all member states of the European Union to ensure that states publish public research results to improve science and strengthen their knowledge-based\cite{reus1}. The recent "Amsterdam Call to Action on Open Sciences" advocates "full open access for all scientific publications" and endorses an environment in which data sharing and management are the default method for all publicly funded research. This document was prepared as an open scientific conference organized by the Dutch Presidency of the Council of the European Union (April 4-5, 2016) \cite{reus2}. Another initiative of the European Commission, which is worth mentioning, is the publication of the FAIR Data Management Guide in Horizon 2020, that is, a set of guidelines for creating data accessible, accessible, compatible and reusable \cite{reus3}. The same as SNSF in "FAIR data principles"\cite{snsf} defines a range of qualities a published dataset should have in order to be Findable, Accessible, Interoperable and Reusable.
Code, data, scenarios and the results of a temporary experiment - provide the means for sharing knowledge about computational analyses.
\subsection{Recommendations}
There are plenty of practices which can be applied to make proper implementation of the Data Publication procedures and, thus, to overcome the impediments to data reuse. Thanos\cite{DBLP:journals/publications/Thanos17} defines most important recommendations:
\begin{itemize}
    \item \textbf{(Meta) Data Modeling.} In order to facilitate data understandability, it is necessary to define and develop formal models that adequately describe:
    \begin{itemize}
        \item data representation needs of a given scientific discipline;
        \item data provenance information;
        \item data contextual information;
        \item data uncertainty;
        \item data quality information.
    \end{itemize}
    All this information is collectively called metadata information. If scientists need to reuse data collected by others, then the data must be carefully documented. Metadata is descriptive information about the data that explains the measured attributes, their names, units, accuracy, accuracy, a location of the data and, ideally, much more. Recently, a mechanism was proposed, a data document capable of improving the understanding of data and, therefore, the reuse of data. A data document can be defined as a scientific publication of a metadata document with a search capability describing a specific on-line available dataset or a group of datasets published by standard academic methods\cite{Chavan2011}.
    \item \textbf{Domain-Specific Ontologies Ontologies.} A key technology that allows a wide range of data transmission services\cite{DBLP:journals/tods/BienvenuCLW14}. The increasing availability of data has shifted focus from closed applications with relatively low data levels, to mechanisms and applications for searching, integrating and using the vast amounts of data that are now available. Ontologies provide a semantic basis that allows you to reuse research data\cite{Gruber:1995:TPD:219666.219701}\cite{DBLP:journals/jods/PoggiLCGLR08}.
    \item \textbf{Data Discovering.} Under the discovery of data, meaning the ability to quickly and accurately identify and find data that supports research requirements. The process of detecting data existing in the data collection/database is supported by a search and query function that uses the capabilities of registering and citing data; and metadata descriptions contained in categorization/classification schemes, data dictionaries, and metadata registries.
    \item \textbf{Data Exchangeability.} By data exchange, we mean the ability of two objects, that is, the author of data and the data user, to exchange meaningful data sets. Data exchange is a prerequisite for the reuse of data. During the data exchange process, especially when data moves between scientific disciplines, it is necessary to solve three types of "heterogeneity".
    \item \textbf{Linking Data to Publications.} In science with a predominance of data, scientific communication undergoes significant changes. Modern scientific communication should support the practice of providing a link to data in the same way that researchers usually provide a bibliographic reference to printed resources. The need for quoting data is beginning to be recognized as one of the critical practices underlying the recognition of data as primary research products and not as a by-product of research
\end{itemize}
\section{Replicability}
The term replicability is not defined in computer science, and most of the time it refers to reproducibility. But for example in psychological scientific research Fournier\cite{Four} explains that the study should give the same results if they are repeated precisely. The replicability can be increased by an in-depth review of other similar experiments and exclude variables that you might not have thought of. \par
At the same time, Sinha\cite{For} gives a different definition that replicability uses the same methods, but with different themes and experimenters,  can also be reproduced someone else's work or results to establish its validity. For example, if someone executes experiment A and reaches conclusion B, and if someone does the same experiment A and come to a C output that is different from B, then it will not replicate because the results will not be reproduced. \par
in the end, Fehr\cite{DBLP:journals/corr/FehrHHS16} gives quite a precise definition of replicability of the attribute describes the possibility of repeating experiments based on calculations and obtaining the same results. Sometimes this is an equivalent term. For replication, required documentation is required on how to run the software to get replicated results. Replicability, in turn, is the primary requirement of reliable software, as well as its results, as it demonstrates the absolute stability of the procedure against statistical influences and observer bias. Also, screening can serve as a benchmark against which new methods can be compared, as indicated in. \par
In this work, replicability can be defined as an ability of scientific work to be replicated by anyone. A researcher needs to provide detailed information and documentation on experiment workflow, codebase wich documentation. The main idea that other researcher can get the same conclusions. This workflow is shown in picture \ref{fig:replicability}.\par
\begin{figure}[h!]
  \includegraphics[scale=0.8]{fig/replicability.png}
  \caption{Replicability\cite{gith}}
  \label{fig:replicability}
\end{figure}
\subsection{Current state of art}
Concerns about reproducibility are confirmed by the observation that replication studies are rare and in some areas become less frequent with time. Various authors have established links between the rarity of replicas and the importance attached to innovation and original research, or to the perception that replications are not published in journals. The Nature study showed that a minority of researchers even attempted to publish replication, although, on a positive note, 24\% published successful replication, and 13\% published a refusal to replicate the result. This is balanced approximately by the equivalent number (10\%) of respondents who reported that they could not issue the error message\cite{DBLP:conf/kolicalling/AhadiHIKP16}
\subsection{Recommendations}
\begin{itemize}
    \item Basic documentation. A fundamental requirement for replicability is the abbreviated documentation, which includes instructions on how to generate an executable binary program in the case of a compiled language and a description of how to run the program to obtain replicated results. This documentation is crucial for the replication of the experiment because it determines the technical implementation and provides a possible repetition of the experiment. Often numerically calculated results are further processed to facilitate interpretation, for example, by visualization. The documentation on the evaluation of these results, descriptively or algorithmically, is necessary for replicating not only the results of calculations but also their evaluation.
    \item Automation and testing. The automation of the experiment makes it possible to quickly and reliably check the possibility of replicating experiments based on calculations. Usually, this means that one or more scenarios automatically prepare and run the experiment, and after processing the results. The reproducible behavior of all building blocks of the experiment, for which it is recommended to set up specific tests. Three categories of tests are usually considered: unittests, small sections of the source code exam; Integration tests, verification of the main components of the source code; and system tests, to evaluate the whole project. Tests usually include comparing a statistically significant sample with the analytical results, or a matching control problem\cite{DBLP:journals/corr/FehrHHS16}.
\end{itemize}

